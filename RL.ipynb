{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeSOSLyw3Nzj3ea6bUXGSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PTrillat/Reinforcement-Learning/blob/main/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe des environnements\n",
        "Par compatibilité avec Gym, ces classe crééront un objet avec deux méthode:\n",
        " - etat = jeu.reset() -> np.ndarray\n",
        " - etat, gain, fin, _ = jeu.step(action)ndarray]"
      ],
      "metadata": {
        "id": "jeVMOdME0H1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "8pzas2-DjKmm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KwNdqwXrzy4C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random as rd\n",
        "\n",
        "\n",
        "class Jeu2048(np.ndarray):\n",
        "  def __new__(cls, m, n, p1, p2):\n",
        "    new = np.zeros((m,n), dtype=int).view(cls)\n",
        "    new.p1 = p1\n",
        "    new.p2 = p2\n",
        "    return new\n",
        "\n",
        "  def reset(self):\n",
        "    self *= 0\n",
        "    self._peupler()\n",
        "    return self._etat()\n",
        "  \n",
        "  def step(self, action):\n",
        "    fin = False\n",
        "    if   action==0: depl, fusion = self._gauche()\n",
        "    elif action==1: depl, fusion = self._droite()\n",
        "    elif action==2: depl, fusion = self._haut()\n",
        "    else:           depl, fusion = self._bas()\n",
        "    if depl: self._peupler()\n",
        "    #else: fusion = -1000\n",
        "    return self._etat(), fusion, self._fin(), 'ich bin ein kartoffel'\n",
        "  \n",
        "  def _fin(self):\n",
        "    copie = 0 + self # un peu sale\n",
        "    if copie._gauche()[0]: return False\n",
        "    if copie._droite()[0]: return False\n",
        "    if copie._haut()[0]: return False\n",
        "    if copie._bas()[0]: return False\n",
        "    return True\n",
        "\n",
        "  def _etat(self):\n",
        "    return np.log2(1*(self==0)+self).flatten()\n",
        "  \n",
        "  def _peupler(self):\n",
        "    cases_vides = list(self._cases_vides())\n",
        "    i, j = cases_vides.pop(rd.randint(len(cases_vides)))\n",
        "    self[i,j] = 2 if rd.rand() < self.p2 else 4\n",
        "    if len(cases_vides) == 0 or rd.rand() < self.p1: return\n",
        "    i, j = cases_vides.pop(rd.randint(len(cases_vides)))\n",
        "    self[i,j] = 2 if rd.rand() < self.p2 else 4\n",
        "  \n",
        "  def _cases_vides(self):\n",
        "    m, n = self.shape\n",
        "    for i in range(m):\n",
        "      for j in range(n):\n",
        "        if self[i,j] == 0: yield (i, j)\n",
        "  \n",
        "  def _gauche(self):\n",
        "    depl, fusion = False, 0\n",
        "    m, n = self.shape\n",
        "    for i in range(m):\n",
        "      j, k = 0, 1\n",
        "      while j<k and k<n:\n",
        "        if self[i,k]==0: # [j=?,...,k=0,...]\n",
        "          k += 1\n",
        "        elif self[i,j]==0: # [j=0,...,k=2,...]\n",
        "          depl = True\n",
        "          self[i,j] = self[i,k]\n",
        "          self[i,k] = 0\n",
        "          k += 1\n",
        "        elif self[i,j]==self[i,k]: # [j=2,...,k=2,...]\n",
        "          depl = True\n",
        "          fusion += self[i,j]\n",
        "          self[i,j] *= 2\n",
        "          self[i,k] = 0\n",
        "          j += 1\n",
        "          k += 1\n",
        "        elif j+1==k: # [j=2,k=4,...]\n",
        "          j += 1\n",
        "          k += 1\n",
        "        else: # [j=2,...,k=4,...]\n",
        "          j += 1\n",
        "    return depl, fusion\n",
        "  \n",
        "  def _droite(self):\n",
        "    self = self[:,::-1]\n",
        "    depl, fusion = self._gauche()\n",
        "    self = self[:,::-1]\n",
        "    return depl, fusion\n",
        "  \n",
        "  def _haut(self):\n",
        "    self = self.T\n",
        "    depl, fusion = self._gauche()\n",
        "    self = self.T\n",
        "    return depl, fusion\n",
        "  \n",
        "  def _bas(self):\n",
        "    self = self[::-1,:]\n",
        "    depl, fusion = self._haut()\n",
        "    self = self[::-1,:]\n",
        "    return depl, fusion\n",
        "\n",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "\n",
        "class Zavatta(CartPoleEnv):\n",
        "  \n",
        "  M = np.diag([1, 1, 1, 1])\n",
        "  \n",
        "  def step(self, action):\n",
        "    state, reward, done, info = super(Zavatta, self).step(action)\n",
        "    return state, 1/(1 + state @ Zavatta.M @ state), done, info\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le plagiat j'adore ça"
      ],
      "metadata": {
        "id": "LommTYK5evN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "#env = Zavatta(); dim_state = 4; dim_action = 2; max_steps = 1000\n",
        "env = Jeu2048(4,4,0.9,0.9); dim_state = env.size; dim_action = 4; max_steps = 700\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  return (state.astype(np.float32), np.array(reward, np.float32), np.array(done, np.int32))\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], [tf.float32, tf.float32, tf.int32])"
      ],
      "metadata": {
        "id": "3TEvLVbve4mc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "  def __init__(\n",
        "      self,  \n",
        "      common_1: int,\n",
        "      common_2: int,\n",
        "      actor_1: int,\n",
        "      actor_2: int,\n",
        "      critic_1: int,\n",
        "      critic_2: int):\n",
        "    super().__init__()\n",
        "    self.common_1 = layers.Dense(common_1, activation=\"relu\")\n",
        "    self.common_2 = layers.Dense(common_2, activation=\"relu\")\n",
        "    self.actor_1 = layers.Dense(actor_1)\n",
        "    self.actor_2 = layers.Dense(actor_2)\n",
        "    self.critic_1 = layers.Dense(critic_1)\n",
        "    self.critic_2 = layers.Dense(critic_2)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common_2(self.common_1(inputs))\n",
        "    return self.actor_2(self.actor_1(x)), self.critic_2(self.critic_1(x))\n",
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_common_1: int,\n",
        "      num_common_2: int,\n",
        "      num_common_3: int,\n",
        "      num_actor_1: int,\n",
        "      num_actor_2: int,\n",
        "      num_actor_3: int,\n",
        "      num_critic_1: int,\n",
        "      num_critic_2: int,\n",
        "      num_critic_3: int):\n",
        "    super().__init__()\n",
        "    self.common_1 = layers.Dense(num_common_1, activation=\"relu\")\n",
        "    self.common_2 = layers.Dense(num_common_2, activation=\"relu\")\n",
        "    self.common_3 = layers.Dense(num_common_3, activation=\"relu\")\n",
        "    self.actor_1 = layers.Dense(num_actor_1, activation=\"relu\")\n",
        "    self.actor_2 = layers.Dense(num_actor_2, activation=\"relu\")\n",
        "    self.actor_3 = layers.Dense(num_actor_3, activation=\"relu\")\n",
        "    self.critic_1 = layers.Dense(num_critic_1, activation=\"relu\")\n",
        "    self.critic_2 = layers.Dense(num_critic_2, activation=\"relu\")\n",
        "    self.critic_3 = layers.Dense(num_critic_3, activation=\"relu\")\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common_3(self.common_2(self.common_1(inputs)))\n",
        "    return self.actor_3(self.actor_2(self.actor_1(x))), self.critic_3(self.critic_2(self.critic_1(x)))\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras.layers import InputLayer, Dense\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "  \n",
        "  def __init__(\n",
        "      self,\n",
        "      num_common: Tuple[int, ...], \n",
        "      num_actor: Tuple[int, ...], \n",
        "      num_critic: Tuple[int, ...]):\n",
        "    \n",
        "    super().__init__()\n",
        "    state, num_common, middle = num_common[0], num_common[1:], num_common[-1]\n",
        "    \n",
        "    input_common = InputLayer(input_shape=(state,))\n",
        "    for num in num_common:\n",
        "      input_common = Dense(units=num, )\n",
        "\n",
        "    units = [32, 16, 8]\n",
        "\n",
        "    for unit in range(len(units)):\n",
        "        inp =  tf.layers.dense(inp, units=units[unit], kernel_initializer=tf.initializers.he_uniform(),activation=tf.nn.relu,name=\"hidden\" + str(unit + 1))\n",
        "        inp = tf.layers.batch_normalization(inputs=inp, name=\"bn\"+str(unit + 1))\n",
        "\n",
        "\n",
        "    out = tf.layers.dense(inp, units=1, kernel_initializer=tf.initializers.he_uniform(), activation=None, name=\"out\")  \n",
        "\n",
        "    self.common = Sequential([Input(shape = (state, ))] + [Dense(num, activation='relu') for num in num_common])\n",
        "    self.actor = Sequential([Input(shape = (middle, ))] + [Dense(num, activation='relu') for num in num_actor])\n",
        "    self.critic = Sequential([Input(shape = (middle, ))] + [Dense(num, activation='relu') for num in num_critic])\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GWuqA1LIet2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3f793f58-c72f-4b7f-c6f3-1ef60802f4ea"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom tensorflow.keras.layers import InputLayer, Dense\\n\\nclass ActorCritic(tf.keras.Model):\\n  \\n  def __init__(\\n      self,\\n      num_common: Tuple[int, ...], \\n      num_actor: Tuple[int, ...], \\n      num_critic: Tuple[int, ...]):\\n    \\n    super().__init__()\\n    state, num_common, middle = num_common[0], num_common[1:], num_common[-1]\\n    \\n    input_common = InputLayer(input_shape=(state,))\\n    for num in num_common:\\n      input_common = Dense(units=num, )\\n\\n    units = [32, 16, 8]\\n\\n    for unit in range(len(units)):\\n        inp =  tf.layers.dense(inp, units=units[unit], kernel_initializer=tf.initializers.he_uniform(),activation=tf.nn.relu,name=\"hidden\" + str(unit + 1))\\n        inp = tf.layers.batch_normalization(inputs=inp, name=\"bn\"+str(unit + 1))\\n\\n\\n    out = tf.layers.dense(inp, units=1, kernel_initializer=tf.initializers.he_uniform(), activation=None, name=\"out\")  \\n\\n    self.common = Sequential([Input(shape = (state, ))] + [Dense(num, activation=\\'relu\\') for num in num_common])\\n    self.actor = Sequential([Input(shape = (middle, ))] + [Dense(num, activation=\\'relu\\') for num in num_actor])\\n    self.critic = Sequential([Input(shape = (middle, ))] + [Dense(num, activation=\\'relu\\') for num in num_critic])\\n\\n  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\\n    x = self.common(inputs)\\n    return self.actor(x), self.critic(x)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ],
      "metadata": {
        "id": "Y51SwNpnfGwa"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expected_return(\n",
        "    values: tf.Tensor,\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float,\n",
        "    lamed: float) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  values = tf.cast(values[::-1], dtype=tf.float32)\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    value = values[i]\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = (1-lamed)*value + lamed*(reward + gamma * discounted_sum)\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  return (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns) + 1e-10)"
      ],
      "metadata": {
        "id": "lRMq-Sp1fKJb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ],
      "metadata": {
        "id": "Un-8MazsiAup"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    lamed: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(values, rewards, gamma, lamed)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "metadata": {
        "id": "m5lWuk7GiFQ-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = ActorCritic(200, 200, 50, dim_action, 50, 1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "max_episodes = 10000\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "lamed = 0.999\n",
        "\n",
        "# Keep last episodes reward\n",
        "serie_gains = deque(maxlen=max_steps)\n",
        "lissage_gain = 0\n",
        "\n",
        "with tqdm.trange(1,max_episodes) as t:\n",
        "  for i in t:\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    gain_episode = train_step(initial_state, model, optimizer, gamma, lamed, max_steps)\n",
        "    \n",
        "    gain_episode = float(gain_episode)\n",
        "    lissage_gain += (gain_episode-lissage_gain)/i\n",
        "    serie_gains.append(gain_episode)\n",
        "      \n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(episode_reward=gain_episode, running_reward=lissage_gain)\n",
        "\n",
        "    #if lissage_gain > 195: break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {lissage_gain:.2f}!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbTOCVKl1C6a",
        "outputId": "4777c099-de4c-4212-c1cb-a900ad3aefed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 1353:  14%|█▎        | 1353/9999 [05:19<52:47,  2.73it/s, episode_reward=332, running_reward=538]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(serie_gains, label='gains instantanés')\n",
        "plt.plot(np.cumsum(serie_gains)/range(1,1+len(serie_gains)), label='gains moyens')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TDtaQDivPjGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "fig = plt.figure() # initialise la figure\n",
        "plt.axis('square')\n",
        "plt.xlim(-2.4, 2.4)\n",
        "plt.ylim(-1, 1)\n",
        "line, = plt.plot([], [])\n",
        "\n",
        "state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "def animate(i):\n",
        "  global state\n",
        "  state = tf.expand_dims(state, 0)\n",
        "  action_probs, _ = model(state)\n",
        "  action = np.argmax(np.squeeze(action_probs))\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  x, x_dot, theta, theta_dot = state\n",
        "  line.set_data([x,x+np.sin(theta)], [0,np.cos(theta)])\n",
        "  return line,\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, frames=300, blit=True, interval=20, repeat=True)\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "Tgpx8Yhs1FN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "initial_state_shape = initial_state.shape\n",
        "state = initial_state\n",
        "for t in tf.range(max_steps):\n",
        "\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.math.argmax(action_logits_t, 1)[0]\n",
        "    \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "\n",
        "    print(env)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break"
      ],
      "metadata": {
        "id": "Fda7D-nCxD4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}