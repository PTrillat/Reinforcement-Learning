{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jeu2048_RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2qi5/OK7bIbI3UnzJ3Pcs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PTrillat/Reinforcement-Learning/blob/main/Jeu2048_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "sZenfs3hGLu3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random as rd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Jeu2048(np.ndarray):\n",
        "  def __new__(cls, m, n, p1, p2):\n",
        "    new = np.zeros((m,n), dtype=int).view(cls)\n",
        "    new.p1 = p1\n",
        "    new.p2 = p2\n",
        "    return new\n",
        "\n",
        "  def maz(self):\n",
        "    self *= 0\n",
        "    self._peupler()\n",
        "    return self._etat()\n",
        "  \n",
        "  def jouer(self, action):\n",
        "    if action == 0: deplacement, gain = self._gauche()\n",
        "    if action == 1: deplacement, gain = self._droite()\n",
        "    if action == 2: deplacement, gain = self._haut()\n",
        "    if action == 3: deplacement, gain = self._bas()\n",
        "    fin = self._peupler() if deplacement else False\n",
        "    return self._etat(), gain, fin\n",
        "  \n",
        "  def _etat(self):\n",
        "    return np.log2(1*(self==0)+self).flatten()\n",
        "  \n",
        "  def _peupler(self):\n",
        "    cases_vides = list(self._cases_vides())\n",
        "    if len(cases_vides) == 0: return True\n",
        "    i, j = cases_vides.pop(rd.randint(len(cases_vides)))\n",
        "    self[i,j] = 2 if rd.rand() < self.p2 else 4\n",
        "    if len(cases_vides) == 0 or rd.rand() < self.p1: return False\n",
        "    i, j = cases_vides.pop(rd.randint(len(cases_vides)))\n",
        "    self[i,j] = 2 if rd.rand() < self.p2 else 4\n",
        "    return False\n",
        "  \n",
        "  def _cases_vides(self):\n",
        "    m, n = self.shape\n",
        "    for i in range(m):\n",
        "      for j in range(n):\n",
        "        if self[i,j] == 0: yield (i, j)\n",
        "  \n",
        "  def _gauche(self):\n",
        "    def fusionner():\n",
        "      self[i,j+1] += self[i,j]\n",
        "      self[i,j:-1] = self[i,j+1:]\n",
        "      self[i,-1] = 0\n",
        "      return True, gain + self[i,j]\n",
        "    deplacement, gain = False, 0\n",
        "    m, n = self.shape\n",
        "    for i in range(m):\n",
        "      j = 0\n",
        "      for iter in range(n-1):\n",
        "        if self[i,j] == 0 or self[i,j+1] == 0: deplacement, gain = fusionner()\n",
        "        else:\n",
        "          if self[i,j] == self[i,j+1]: deplacement, gain = fusionner()\n",
        "          j += 1\n",
        "    return deplacement, gain\n",
        "  \n",
        "  def _droite(self):\n",
        "    self = self[:,::-1]\n",
        "    deplacement, gain = self._gauche()\n",
        "    self = self[:,::-1]\n",
        "    return deplacement, gain\n",
        "  \n",
        "  def _haut(self):\n",
        "    self = self.T\n",
        "    deplacement, gain = self._gauche()\n",
        "    self = self.T\n",
        "    return deplacement, gain\n",
        "  \n",
        "  def _bas(self):\n",
        "    self = self[::-1,:]\n",
        "    deplacement, gain = self._haut()\n",
        "    self = self[::-1,:]\n",
        "    return deplacement, gain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "# petit détaille : tf.function est un décorateur transformant les arguments d'une fonction en tenseur\n",
        "# on ne peut donc pas décorer directement Jeu2048.jouer(self, action) à cause du self...\n",
        "# pour le retirer, on utilisera donc une variable globale... (oui c'est moche)\n",
        "\n",
        "grille = Jeu2048(4, 4, 0.25, 0.25)\n",
        "\n",
        "def grille_maz() -> np.ndarray:\n",
        "  global grille\n",
        "  state = grille.maz()\n",
        "  return state.astype(np.float32)\n",
        "\n",
        "def grille_jouer(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  global grille\n",
        "  state, reward, done = grille.jouer(action)\n",
        "  return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
        "\n",
        "def tf_grille_maz() -> tf.Tensor:\n",
        "  return tf.numpy_function(grille_maz, [], tf.float32)\n",
        "\n",
        "def tf_grille_jouer(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(grille_jouer, [action], [tf.float32, tf.int32, tf.int32])"
      ],
      "metadata": {
        "id": "7_2R0Oy8wMVQ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "  def __init__(self, entree, shape_common, shape_actor, action, shape_critic):\n",
        "    super().__init__()\n",
        "    # common entree -> shape_common -> intermédiaire\n",
        "    self.common = tf.keras.models.Sequential()\n",
        "    self.common.add(tf.keras.Input(shape=(entree,)))\n",
        "    for num in shape_common: self.common.add(tf.keras.layers.Dense(num, activation='relu'))\n",
        "    intermedaire = num\n",
        "    # common intermédiaire -> shape_actor -> action\n",
        "    self.actor = tf.keras.models.Sequential()\n",
        "    self.actor.add(tf.keras.Input(shape=(intermedaire,)))\n",
        "    for num in shape_actor: self.actor.add(tf.keras.layers.Dense(num, activation='relu'))\n",
        "    self.actor.add(tf.keras.layers.Dense(action, activation=tf.keras.activations.softmax))\n",
        "    # common intermédiaire -> shape_critic -> 1\n",
        "    self.critic = tf.keras.models.Sequential()\n",
        "    self.critic.add(tf.keras.Input(shape=(intermedaire,)))\n",
        "    for num in shape_critic: self.critic.add(tf.keras.layers.Dense(num, activation='relu'))\n",
        "    self.critic.add(tf.keras.layers.Dense(1, activation=tf.keras.activations.softmax))\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)\n",
        "\n",
        "def run_episode(model, max_steps):\n",
        "  # Type particulier de tf efficace pour l'ajout à la fin\n",
        "  Probas = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  Values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  Rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "  state = tf_grille_maz()\n",
        "  for t in tf.range(max_steps):\n",
        "    state = tf.expand_dims(state, 0) # Convert state into a batched tensor (batch size = 1)\n",
        "    probas, value = model(state) # Run the model to get action probabilities and critic value\n",
        "    action = tf.random.categorical(tf.math.log(probas), 1)[0,0] # Sample next action from the action probability distribution\n",
        "    state, reward, done = tf_grille_jouer(action) # Apply action to the environment to get next state and reward\n",
        "    Probas = Probas.write(t, tf.squeeze(probas))\n",
        "    Values = Values.write(t, tf.squeeze(value)) # Store log probability of the action chosen\n",
        "    Rewards = Rewards.write(t, reward) # Store reward\n",
        "    if tf.cast(done, tf.bool): break\n",
        "  Probas = Probas.stack()\n",
        "  Values = Values.stack()\n",
        "  Rewards = Rewards.stack()\n",
        "  return Probas, Values, Rewards\n",
        "\n",
        "def get_gains(Values, Rewards, gamma, lamed):\n",
        "  n = tf.shape(Rewards)[0]\n",
        "  Rewards = tf.cast(Rewards, dtype=tf.float32)\n",
        "  Gains = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "  gain = Values[-1]\n",
        "  for t in range(n-1,-1,-1):\n",
        "    gain = (1-lamed)*Values[t] + lamed*(Rewards[t] + gamma*gain)\n",
        "    Gains = Gains.write(t, gain)\n",
        "  Gains = Gains.stack()\n",
        "  #Gains = (Gains - tf.math.reduce_mean(Gains)) / (tf.math.reduce_std(Gains) + 1e-5)\n",
        "  return Gains\n",
        "\n",
        "def compute_loss(Probas, Values, Gains):\n",
        "  Avantages = tf.expand_dims(Gains - Values, 1)\n",
        "  actor_loss = -tf.math.reduce_sum(tf.math.log(Probas)*Avantages)\n",
        "  critic_loss = huber_loss(Values, Gains)\n",
        "  return actor_loss + critic_loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(model: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, gamma: float, lamed: float, max_steps: int) -> tf.Tensor:\n",
        "  with tf.GradientTape() as tape:\n",
        "    Probas, Values, Rewards = run_episode(model, max_steps)\n",
        "    Gains = get_gains(Values, Rewards, gamma, lamed)\n",
        "    loss = compute_loss(Probas, Values, Gains)\n",
        "  grads = tape.gradient(loss, model.trainable_variables) # Compute the gradients from the loss\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables)) # Apply the gradients to the model's parameters\n",
        "  episode_reward = tf.math.reduce_sum(Rewards)\n",
        "  return episode_reward"
      ],
      "metadata": {
        "id": "yi2eL6356M9U"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "cerveau = ActorCritic(grille.size, (16,16), (), 4, ())\n",
        "\n",
        "from collections import deque\n",
        "import tqdm\n",
        "\n",
        "gamma = 0.999\n",
        "lamed = 0.8\n",
        "num_episodes = 500\n",
        "min_steps = 100\n",
        "max_steps = 5000\n",
        "\n",
        "rewards = deque(maxlen=num_episodes)\n",
        "maxima = deque(maxlen=num_episodes)\n",
        "\n",
        "with tqdm.trange(num_episodes) as t:\n",
        "  for i in t:\n",
        "    reward = train_step(cerveau, optimizer, gamma, lamed, int(min_steps + i/num_episodes*(max_steps-min_steps)))\n",
        "    maximum = np.max(grille)\n",
        "    reward = int(reward)\n",
        "    rewards.append(reward)\n",
        "    maxima.append(maximum)\n",
        "      \n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(episode_reward=reward, maximum=maximum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt7Vk19uiJoY",
        "outputId": "3d1f0b7f-18ce-486d-eefb-e00691be2a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 3:   1%|          | 4/500 [00:12<20:31,  2.48s/it, episode_reward=1008, maximum=64]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7fb133246a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 4:   1%|          | 5/500 [00:13<18:27,  2.24s/it, episode_reward=1052, maximum=64]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7fb133246a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rewards, label='gain instantané')\n",
        "plt.plot(np.cumsum(rewards)/range(1,1+len(rewards)), label='gain moyen')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(maxima, label='tuile maximum')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "86WvkFdyuqeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}